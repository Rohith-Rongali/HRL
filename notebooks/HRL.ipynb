{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P_DODRgW_ZKS"
      },
      "outputs": [],
      "source": [
        "\n",
        "import numpy as np\n",
        "import random\n",
        "import gym\n",
        "from gym.wrappers import Monitor\n",
        "import glob\n",
        "import io\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython.display import HTML\n",
        "import pandas as pd\n",
        "from IPython import display\n",
        "from IPython.display import clear_output\n",
        "from tqdm import tqdm\n",
        "import time\n",
        "from IPython.display import display, clear_output\n",
        "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
        "import matplotlib.patches as patches"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fYNA5kiH_esJ",
        "outputId": "d850265c-7c0b-46d9-cc67-ea93853162e6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "269\n",
            "Number of states: 500\n",
            "Number of actions that an agent can take: 6\n",
            "Action taken: west\n",
            "Transition probability: {'prob': 1.0}\n",
            "Next state: 249\n",
            "Reward recieved: -1\n",
            "Terminal state: False\n",
            "+---------+\n",
            "|R: | : :\u001b[35mG\u001b[0m|\n",
            "| : | : : |\n",
            "| : :\u001b[43m \u001b[0m: : |\n",
            "| | : | : |\n",
            "|\u001b[34;1mY\u001b[0m| : |B: |\n",
            "+---------+\n",
            "  (West)\n"
          ]
        }
      ],
      "source": [
        "\n",
        "#Setting up the environment\n",
        "env = gym.make('Taxi-v3')\n",
        "\n",
        "env.reset()\n",
        "\n",
        "#Current State\n",
        "print(env.s)\n",
        "\n",
        "print (\"Number of states:\", env.nS)\n",
        "\n",
        "# Primitive Actions\n",
        "action = [\"south\", \"north\", \"east\", \"west\",\"pick\",\"drop\"]\n",
        "#correspond to [0,1,2,3] that's actually passed to the environment\n",
        "\n",
        "# R, G, Y, B, 0 , 1, 2, 3 in state decoding\n",
        "# either go left, up, down or right, pickup or dropoff\n",
        "print (\"Number of actions that an agent can take:\", env.nA)\n",
        "\n",
        "# Example Transitions\n",
        "rnd_action = random.randint(0, 5)\n",
        "print (\"Action taken:\", action[rnd_action])\n",
        "next_state, reward, is_terminal, t_prob = env.step(rnd_action)\n",
        "print (\"Transition probability:\", t_prob)\n",
        "print (\"Next state:\", next_state)\n",
        "print (\"Reward recieved:\", reward)\n",
        "print (\"Terminal state:\", is_terminal)\n",
        "env.render()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zSceh_4NHWYt",
        "outputId": "62b27837-400a-4a6a-f1ab-b779c33b4a46"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[0, 4, 2, 1]\n",
            "+---------+\n",
            "|R: | : :\u001b[35m\u001b[43mG\u001b[0m\u001b[0m|\n",
            "| : | : : |\n",
            "| : : : : |\n",
            "| | : | : |\n",
            "|\u001b[34;1mY\u001b[0m| : |B: |\n",
            "+---------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "env.reset()\n",
        "env.step(0)\n",
        "\n",
        "env.reset()\n",
        "# print(list(env.decode(env.s))[:2])\n",
        "print(list(env.decode(env.s)))\n",
        "env.render()\n",
        "R = [0,0]\n",
        "G = [0,4]\n",
        "Y = [4,0]\n",
        "B = [4,3]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2p77FH-UwUmN",
        "outputId": "e020c20d-5506-424b-869a-f7e4b3f97949"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+---------+\n",
            "|R: | : :\u001b[35m\u001b[43mG\u001b[0m\u001b[0m|\n",
            "| : | : : |\n",
            "| : : : : |\n",
            "| | : | : |\n",
            "|\u001b[34;1mY\u001b[0m| : |B: |\n",
            "+---------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "env.render()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vc5DlXOhlX7l"
      },
      "source": [
        "We consider the goal state to be either of the four R,G,Y,B. We need a data structure to hold these 4 Q-tables. \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9hacJEN8R_Bb"
      },
      "outputs": [],
      "source": [
        "\n",
        "seed = 42\n",
        "rg = np.random.RandomState(seed)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fzsCPav3p3c7"
      },
      "outputs": [],
      "source": [
        "def plot_Q(Q, message = \"Q plot\"):\n",
        "    \n",
        "    # plt.figure(figsize=(10,10))\n",
        "    fig, ax = plt.subplots(figsize = (10,10))\n",
        "\n",
        "    Q_max = Q.max(-1)\n",
        "    \n",
        "    im = ax.imshow(Q_max)\n",
        "    \n",
        "\n",
        "    \n",
        "    cbar = ax.figure.colorbar(im, ax=ax)\n",
        "    \n",
        "    ax.set_title(message)\n",
        "    #ax.grid(visible = True, which=\"major\", color=\"w\", linestyle='-', linewidth=2)\n",
        "    ax.set_xlim([0, 5])\n",
        "    ax.set_ylim([0,5])\n",
        "    ax.pcolor(Q_max, edgecolors='k', linewidths=1)\n",
        "    #plt.colorbar()\n",
        "    \n",
        "    # plt.colorbar()\n",
        "    def x_direct(a):\n",
        "        if a in [0, 1]:\n",
        "            return 0\n",
        "        return 1 if a == 2 else -1\n",
        "    def y_direct(a):\n",
        "        if a in [2, 3]:\n",
        "            return 0\n",
        "        return 1 if a == 1 else -1\n",
        "    policy = Q.argmax(-1)\n",
        "    policyx = np.vectorize(x_direct)(policy)\n",
        "    policyy = np.vectorize(y_direct)(policy)\n",
        "    idx = np.indices(policy.shape)\n",
        "    ax.quiver(idx[1].ravel()+ 0.5, idx[0].ravel()+0.5, policyx.ravel(), policyy.ravel(), pivot=\"middle\", color='red')\n",
        "    fig.tight_layout()\n",
        "\n",
        "def q_learning(env,goal_state, episodes = 1000, alpha0 = 0.4,  epsilon0 = 0.2, beta = 1, gamma = 0.9, plot_heat = True, print_freq = 100, max_steps = 200):\n",
        "\n",
        "\n",
        "    Q = np.zeros((5,5, 4))  #only first 4 actions allowed\n",
        "\n",
        "    episode_rewards = np.zeros(episodes)\n",
        "    steps_to_completion = np.zeros(episodes)\n",
        "    '''if plot_heat:\n",
        "        clear_output(wait=True)\n",
        "        plot_Q(Q)'''\n",
        "    epsilon = epsilon0\n",
        "    alpha = alpha0\n",
        "    for ep in tqdm(range(episodes)):\n",
        "        tot_reward, steps = 0, 0\n",
        "        \n",
        "        # Reset environment\n",
        "        state_seq = env.reset()\n",
        "        state = list(env.decode(state_seq))[:2]\n",
        "\n",
        "        done = False\n",
        "        while not done:\n",
        "            action = egreedy_policy(Q,state,epsilon)\n",
        "\n",
        "            state_next_seq, reward ,_,_ = env.step(action)\n",
        "\n",
        "            #action_next = np.argmax(Q[state_next])\n",
        "            state_next = list(env.decode(state_next_seq))[:2]\n",
        "\n",
        "            if(state_next == state):\n",
        "              reward += -20\n",
        "\n",
        "            if(state_next == goal_state):\n",
        "              reward += 100\n",
        "            \n",
        "            \n",
        "            # update equation\n",
        "            Q[state[0],state[1], action] += alpha*(reward + gamma*np.max(Q[state_next[0],state_next[1]]) - Q[state[0],state[1], action])\n",
        "                                                    \n",
        "            tot_reward += reward\n",
        "            steps += 1\n",
        "\n",
        "            #How do we know the state index of R,G,Y,B?\n",
        "            if steps == max_steps or state_next == goal_state : done =True\n",
        "            \n",
        "            state = state_next\n",
        "            \n",
        "        \n",
        "        episode_rewards[ep] = tot_reward\n",
        "        steps_to_completion[ep] = steps\n",
        "\n",
        "        \n",
        "        #if (ep+1)%print_freq == 0 and plot_heat:\n",
        "    clear_output(wait=True)\n",
        "    plot_Q(Q, message = \"Episode %d: Reward: %f, Steps: %.2f, Qmax: %.2f, Qmin: %.2f\"%(ep+1, np.mean(episode_rewards[ep-print_freq+1:ep]),\n",
        "                                                                           np.mean(steps_to_completion[ep-print_freq+1:ep]),\n",
        "                                                                           Q.max(), Q.min()))\n",
        "    #display.display( df.T )  \n",
        "    return Q, episode_rewards, steps_to_completion, env"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eQhVALqHRYyE"
      },
      "outputs": [],
      "source": [
        "#0-\"south\"\n",
        "#1- \"north\"\n",
        "#2- \"east\"\n",
        "#3- \"west\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "481RBgNvRYvf"
      },
      "outputs": [],
      "source": [
        "from IPython.display import Image\n",
        "# alpha = 0.4\n",
        "alpha=  0.19751633358692788\n",
        "epsilon = 0.12233772892858322\n",
        "gamma = 0.9684975038393936\n",
        "\n",
        "R_mat = np.array([[1,3,0,0,0],\n",
        "                  [1,1,0,0,0],\n",
        "                  [1,1,3,3,3],\n",
        "                  [1,1,1,1,1],\n",
        "                  [1,1,1,1,1]])\n",
        "#Image('/content/option_R.png')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ealWsIQc_C6x"
      },
      "outputs": [],
      "source": [
        "\n",
        "Y_mat = np.array([[0,0,0,0,0],\n",
        "                  [0,0,0,0,0],\n",
        "                  [0,3,3,3,3],\n",
        "                  [0,1,1,1,1],\n",
        "                  [0,1,1,1,1]])\n",
        "#Image('/content/option_Y.png')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "01QGhL-__CrG"
      },
      "outputs": [],
      "source": [
        "\n",
        "G_mat = np.array([[0,0,2,2,1],\n",
        "                  [0,0,1,1,1],\n",
        "                  [2,2,2,2,1],\n",
        "                  [1,1,1,1,1],\n",
        "                  [1,1,1,1,1]])\n",
        "#Image('/content/option_G.png')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "12y1PdViRYsa"
      },
      "outputs": [],
      "source": [
        "\n",
        "B_mat = np.array([[0,0,0,0,0],\n",
        "                  [0,0,0,0,0],\n",
        "                  [2,2,2,0,0],\n",
        "                  [1,1,1,0,0],\n",
        "                  [1,1,1,0,3]])\n",
        "#Image('/content/option_B.png')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g4MRC1p2DZbp"
      },
      "outputs": [],
      "source": [
        "# We are defining four more options here\n",
        "# Option 1 move_to_R \n",
        "# Option 2 move_to_G\n",
        "# Option 3 move_to_Y\n",
        "# Option 4 move_to_B  \n",
        "\n",
        "def move_to_R(env,state):\n",
        "    \n",
        "    ds = list(env.decode(state))[:2]\n",
        "    optdone = False\n",
        "    optact = R_mat[ds[0],ds[1]]\n",
        "    \n",
        "    if (ds == [0,0]):    #termination condition\n",
        "        optdone = True\n",
        "    \n",
        "    return [optact,optdone]\n",
        "    \n",
        "def move_to_Y(env,state):\n",
        "    \n",
        "    ds = list(env.decode(state))[:2]\n",
        "    optdone = False\n",
        "    optact = Y_mat[ds[0],ds[1]]\n",
        "    \n",
        "    if (ds == [4,0]):     #termination condition\n",
        "        optdone = True\n",
        "    \n",
        "    return [optact,optdone]\n",
        "    \n",
        "def move_to_G(env,state):\n",
        "\n",
        "    ds = list(env.decode(state))[:2]\n",
        "    optdone = False\n",
        "    optact = G_mat[ds[0],ds[1]]\n",
        "    \n",
        "    if (ds == [0,4]):     #termination condition\n",
        "        optdone = True\n",
        "    \n",
        "    return [optact,optdone]\n",
        "\n",
        "def move_to_B(env,state):\n",
        "\n",
        "    ds = list(env.decode(state))[:2]\n",
        "    optdone = False\n",
        "    optact = B_mat[ds[0],ds[1]]\n",
        "    \n",
        "    if (ds == [4,3]):     #termination condition\n",
        "        optdone = True\n",
        "    \n",
        "    return [optact,optdone]\n",
        "    \n",
        "\n",
        "#Now the new action space will contain\n",
        "#Primitive Actions: [\"south\", \"north\", \"east\", \"west\",\"pick\",\"drop\"]\n",
        "#Options: [\"move_to_R\",\"move_to_Y\",\"move_to_G\",\"move_to_B\"]\n",
        "#Total Actions :[\"south\", \"north\", \"east\", \"west\",\"pick\",\"drop\",\"move_to_R\",\"move_to_Y\",\"move_to_G\",\"move_to_B\"]\n",
        "#Corresponding to [0,1,2,3,4,5,6,7,8,9]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bh_oghc7Ledh"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "actions=[0,1,2,3,4,5,6,7,8,9]\n",
        "#epsilon-greedy action selection function\n",
        "seed = 36\n",
        "rg = np.random.RandomState(seed)\n",
        "\n",
        "def egreedy_policy(q_values,state,epsilon):\n",
        "    if rg.rand() < epsilon:\n",
        "        return rg.choice(actions)\n",
        "    else:\n",
        "        #max = np.max(q_values[state]) \n",
        "        #return rg.choice(np.where(q_values[state] == max)[0])\n",
        "        return np.argmax(q_values[state])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xtSFMQyawUmi"
      },
      "outputs": [],
      "source": [
        "# sweep_config = {\n",
        "#     \"name\" : \"SMDP-sweep\",\n",
        "#     \"method\": \"random\",\n",
        "#     \"parameters\": {\n",
        "#         \"gamma\": {\n",
        "#             \"min\": 0.850,\n",
        "#             \"max\": 0.999\n",
        "#         },\n",
        "#         \"alpha\": {\n",
        "#             \"min\": 0.1,\n",
        "#             \"max\": 0.4\n",
        "#         },\n",
        "#         \"epsilon\": {\n",
        "#             \"min\": 0.1,\n",
        "#             \"max\": 0.3\n",
        "#         }\n",
        "#     }\n",
        "# }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yReIXZH5wUmj"
      },
      "outputs": [],
      "source": [
        "#sweep_id = wandb.sweep(sweep_config, project='RLPA3')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OCkqsZzGwUmj"
      },
      "outputs": [],
      "source": [
        "#wandb.agent(sweep_id, SMDP, count=20)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ok_5eQM7OCTj"
      },
      "outputs": [],
      "source": [
        "#### SMDP Q-Learning \n",
        "\n",
        "# Add parameters you might need here\n",
        "# gamma = 0.9\n",
        "# alpha = 0.4\n",
        "alpha1 =  0.2821687004024015\n",
        "epsilon1= 0.13732834786589931\n",
        "gamma1 = 0.9756392557916136\n",
        "def SMDP(gamma, alpha, epsilon):\n",
        "    # wandb.init(project = 'RLPA3', entity = 'reinforce-boys')\n",
        "    # gamma = wandb.config.gamma\n",
        "    # alpha = wandb.config.alpha\n",
        "    # epsilon = wandb.config.epsilon\n",
        "\n",
        "    # Iterate over 1000 episodes\n",
        "    q_values_SMDP = np.zeros((500,10))\n",
        "    ufd1 = np.zeros((500,10))#Update_Frequency Data structure\n",
        "    Rewards = []\n",
        "    for _ in range(10000):\n",
        "        state = env.reset()    \n",
        "        done = False\n",
        "        \n",
        "        # While episode is not over\n",
        "        episode_reward = 0\n",
        "        while not done:\n",
        "            \n",
        "            # Choose action        \n",
        "            action = egreedy_policy(q_values_SMDP, state, epsilon=epsilon)\n",
        "            \n",
        "            # Checking if primitive action\n",
        "            if action < 6:\n",
        "                # Perform regular Q-Learning update for state-action pair\n",
        "\n",
        "                next_state, reward, done,_ = env.step(action)\n",
        "                q_values_SMDP[state, action] += alpha*(reward + gamma*np.max([q_values_SMDP[next_state, action] for action in actions]) - q_values_SMDP[state, action])\n",
        "                ufd1[state,action] += 1\n",
        "                state = next_state\n",
        "                episode_reward += reward\n",
        "\n",
        "            # Checking if action chosen is an option\n",
        "            reward_bar = 0\n",
        "            if action == 6: # action => move_to_R\n",
        "                \n",
        "                initial_state = np.copy(state)\n",
        "                optdone = False\n",
        "                count=0\n",
        "                while (optdone == False):\n",
        "                    \n",
        "                    optact,_ = move_to_R(env,state) \n",
        "                    #\n",
        "                    next_state, reward, done,_ = env.step(optact)\n",
        "\n",
        "                    _,optdone = move_to_R(env,next_state) \n",
        "                    \n",
        "                    reward_bar = reward_bar +  (gamma**count)*reward\n",
        "                    count+=1\n",
        "                    state = next_state\n",
        "                    episode_reward += reward\n",
        "\n",
        "                q_values_SMDP[initial_state, action] += alpha*(reward_bar + (gamma**count)*np.max([q_values_SMDP[state, action] for action in actions]) - q_values_SMDP[initial_state, action])\n",
        "                ufd1[initial_state,action] += 1\n",
        "                \n",
        "            \n",
        "            if action == 7: # action => move_to_Y option\n",
        "\n",
        "                initial_state = np.copy(state)\n",
        "                optdone = False\n",
        "                count=0\n",
        "                while (optdone == False):\n",
        "                    \n",
        "                    optact,_ = move_to_Y(env,state) \n",
        "\n",
        "                    next_state, reward, done,_ = env.step(optact)\n",
        "\n",
        "                    _,optdone = move_to_Y(env,next_state) \n",
        "\n",
        "                    reward_bar = reward_bar +  (gamma**count)*reward\n",
        "                    count+=1\n",
        "    \n",
        "                    state = next_state\n",
        "                    episode_reward += reward\n",
        "\n",
        "                q_values_SMDP[initial_state, action] += alpha*(reward_bar + (gamma**count)*np.max([q_values_SMDP[state, action] for action in actions]) - q_values_SMDP[initial_state, action])\n",
        "                ufd1[initial_state,action] += 1\n",
        "\n",
        "\n",
        "            if action == 8: # action => move_to_G option\n",
        "\n",
        "                initial_state = np.copy(state)\n",
        "                optdone = False\n",
        "                count=0\n",
        "                while (optdone == False):\n",
        "                    \n",
        "                    optact,_ = move_to_G(env,state) \n",
        "                    #\n",
        "                    next_state, reward, done,_ = env.step(optact)\n",
        "\n",
        "                    \n",
        "                    _,optdone = move_to_G(env,next_state) \n",
        "                    \n",
        "                    reward_bar = reward_bar +  (gamma**count)*reward\n",
        "                    count+=1\n",
        "\n",
        "                    state = next_state\n",
        "                    episode_reward += reward\n",
        "\n",
        "                q_values_SMDP[initial_state, action] += alpha*(reward_bar + (gamma**count)*np.max([q_values_SMDP[state, action] for action in actions]) - q_values_SMDP[initial_state, action])\n",
        "                ufd1[initial_state,action] += 1\n",
        "\n",
        "\n",
        "            if action == 9: # action => move_to_B option\n",
        "\n",
        "                initial_state = np.copy(state)\n",
        "                optdone = False\n",
        "                count=0\n",
        "                while (optdone == False):\n",
        "                    \n",
        "                    optact,_ = move_to_B(env,state) \n",
        "                    #\n",
        "                    next_state, reward, done,_ = env.step(optact)\n",
        "\n",
        "                    \n",
        "                    _,optdone = move_to_B(env,next_state) \n",
        "                    \n",
        "                    reward_bar = reward_bar +  (gamma**count)*reward\n",
        "                    count+=1\n",
        "\n",
        "                    state = next_state\n",
        "                    episode_reward += reward\n",
        "\n",
        "                q_values_SMDP[initial_state, action] += alpha*(reward_bar + (gamma**count)*np.max([q_values_SMDP[state, action] for action in actions]) - q_values_SMDP[initial_state, action])\n",
        "                ufd1[initial_state,action] += 1\n",
        "\n",
        "        #wandb.log({'episode_reward': episode_reward})\n",
        "        Rewards.append(episode_reward)\n",
        "    return q_values_SMDP, Rewards, ufd1\n",
        "  \n",
        "\n",
        "\n",
        "#plt.plot(Rewards)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "klUCZLF8wUmk"
      },
      "outputs": [],
      "source": [
        "q_values_SMDP, Rewards, ufd1 = SMDP(gamma1, alpha1, epsilon1)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "np.mean(Rewards[2000:])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CYSVh-7AC4kN",
        "outputId": "25d41675-c15d-4e9c-dc18-5704b0ff58b3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "7.082"
            ]
          },
          "metadata": {},
          "execution_count": 144
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "skwPb-XTwUml"
      },
      "outputs": [],
      "source": [
        "pass_loc = ['R','G','Y','B','in_taxi']\n",
        "def render_taxi(s):\n",
        "    env.s = s\n",
        "    #plt.figure(3)\n",
        "    #plt.clf()\n",
        "    #plt.imshow(env.render(mode='rgb_array'))\n",
        "    env.render()\n",
        "    #display.clear_output(wait=True)\n",
        "    #display.display(plt.gcf())\n",
        "    #env.render()\n",
        "    time.sleep(2)\n",
        "    clear_output(wait = True)\n",
        "\n",
        "def drive_taxi(q_values):\n",
        "    state = env.reset()\n",
        "    done = False\n",
        "    env.render()\n",
        "    clear_output(wait = True)\n",
        "    print(\"Passenger location : \"+pass_loc[list(env.decode(env.s))[2]])\n",
        "    print(\"Destination : \"+pass_loc[list(env.decode(env.s))[3]])\n",
        "    while not done:\n",
        "        \n",
        "        action = np.argmax([q_values[state]])\n",
        "        if action < 6:\n",
        "            state, reward, done,_ = env.step(action)\n",
        "            render_taxi(state)\n",
        "        if action == 6:\n",
        "            optdone = False\n",
        "            while(optdone == False):\n",
        "                optact, optdone = move_to_R(env,state)\n",
        "                next_state, reward, done,_ = env.step(optact)\n",
        "                state = next_state\n",
        "                render_taxi(state)\n",
        "        if action == 7:\n",
        "            optdone = False\n",
        "            while(optdone == False):\n",
        "                optact, optdone = move_to_Y(env,state)\n",
        "                next_state, reward, done,_ = env.step(optact)\n",
        "                state = next_state\n",
        "                render_taxi(state)\n",
        "        if action == 8:\n",
        "            optdone = False\n",
        "            while(optdone == False):\n",
        "                optact, optdone = move_to_G(env,state)\n",
        "                next_state, reward, done,_ = env.step(optact)\n",
        "                state = next_state\n",
        "                render_taxi(state)\n",
        "        if action == 9:\n",
        "            optdone = False\n",
        "            while(optdone == False):\n",
        "                optact, optdone = move_to_B(env,state)\n",
        "                next_state, reward, done,_ = env.step(optact)\n",
        "                state = next_state\n",
        "                render_taxi(state)\n",
        "    \n",
        "            "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sDMyaixqwUml",
        "outputId": "525b6111-b5a1-4d60-e953-12d78dfbd157"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+\n",
            "|\u001b[35m\u001b[34;1m\u001b[43mR\u001b[0m\u001b[0m\u001b[0m: | : :G|\n",
            "| : | : : |\n",
            "| : : : : |\n",
            "| | : | : |\n",
            "|Y| : |B: |\n",
            "+---------+\n",
            "  (Dropoff)\n"
          ]
        }
      ],
      "source": [
        "#img = plt.imshow(env.render(mode = 'rgb_array'))\n",
        "drive_taxi(q_values_SMDP)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lr3uaXRswUmm"
      },
      "outputs": [],
      "source": [
        "# sweep_config = {\n",
        "#     \"name\" : \"Intra-sweep\",\n",
        "#     \"method\": \"random\",\n",
        "#     \"parameters\": {\n",
        "#         \"gamma\": {\n",
        "#             \"min\": 0.850,\n",
        "#             \"max\": 0.999\n",
        "#         },\n",
        "#         \"alpha\": {\n",
        "#             \"min\": 0.1,\n",
        "#             \"max\": 0.4\n",
        "#         },\n",
        "#         \"epsilon\": {\n",
        "#             \"min\": 0.1,\n",
        "#             \"max\": 0.3\n",
        "#         }\n",
        "#     }\n",
        "# }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1C8pfVQDwUmm"
      },
      "outputs": [],
      "source": [
        "#sweep_id = wandb.sweep(sweep_config, project='RLPA3')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I22YjVGOwUmn"
      },
      "outputs": [],
      "source": [
        "#wandb.agent(sweep_id, SMDP, count=20)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r6A2TdUHWVUN"
      },
      "outputs": [],
      "source": [
        "#### Intra-Option Q-Learning \n",
        "\n",
        "# Add parameters you might need here\n",
        "# gamma = 0.9\n",
        "# alpha = 0.4\n",
        "alpha=  0.19751633358692788\n",
        "epsilon = 0.12233772892858322\n",
        "gamma = 0.9684975038393936\n",
        "def Intra(gamma, alpha, epsilon):\n",
        "  # Iterate over 1000 episodes\n",
        "  # gamma = wandb.config.gamma\n",
        "  # alpha = wandb.config.alpha\n",
        "  # epsilon = wandb.config.epsilon\n",
        "  Rewards = []\n",
        "  q_values_SMDP2 = np.zeros((500,10))\n",
        "\n",
        "  ufd2 = np.zeros((500,10))#Update_Frequency Data structure\n",
        "  for _ in range(10000):\n",
        "      state = env.reset()    \n",
        "      done = False\n",
        "\n",
        "      # While episode is not over\n",
        "      episode_reward = 0\n",
        "      while not done:\n",
        "\n",
        "          # Choose action        \n",
        "          action = egreedy_policy(q_values_SMDP2, state, epsilon=epsilon)\n",
        "          \n",
        "          # Checking if primitive action\n",
        "          if action < 6:\n",
        "              # Perform regular Q-Learning update for state-action pair\n",
        "\n",
        "              next_state, reward, done,_ = env.step(action)\n",
        "              q_values_SMDP2[state, action] += alpha*(reward + gamma*np.max([q_values_SMDP2[next_state, action] for action in actions]) - q_values_SMDP2[state, action])\n",
        "              ufd2[state,action] += 1\n",
        "              episode_reward+=reward\n",
        "\n",
        "              state = next_state\n",
        "          \n",
        "          # Checking if action chosen is an option\n",
        "\n",
        "          if action == 6: # action => Move to R option\n",
        "\n",
        "              optdone = False\n",
        "              while (optdone == False) :\n",
        "                  \n",
        "                  optact,_ = move_to_R(env,state) \n",
        "                  next_state, reward, done,_ = env.step(optact)\n",
        "                  _,optdone = move_to_R(env,next_state) \n",
        "                  episode_reward+=reward\n",
        "\n",
        "                  q_values_SMDP2[state, optact] += alpha*(reward + gamma*np.max([q_values_SMDP2[next_state, action] for action in actions]) - q_values_SMDP2[state, optact])\n",
        "                  ufd2[state,optact] += 1\n",
        "                  \n",
        "                  if not optdone:\n",
        "                    q_values_SMDP2[state, action] += alpha*(reward + gamma*q_values_SMDP2[next_state, action] - q_values_SMDP2[state, action])\n",
        "                    ufd2[state,action] += 1\n",
        "                  else:\n",
        "                    q_values_SMDP2[state, action] += alpha*(reward + gamma*np.max([q_values_SMDP2[next_state, action] for action in actions]) - q_values_SMDP2[state, action])\n",
        "                    ufd2[state,action] += 1\n",
        "\n",
        "                  state = next_state\n",
        "\n",
        "              \n",
        "\n",
        "                \n",
        "            \n",
        "          if action == 7: # action => move to Y option\n",
        "\n",
        "              optdone = False\n",
        "              while (optdone == False) :\n",
        "                  \n",
        "                  optact,_ = move_to_Y(env,state) \n",
        "                  next_state, reward, done,_ = env.step(optact)\n",
        "                  _,optdone = move_to_Y(env,next_state) \n",
        "                  episode_reward+=reward\n",
        "\n",
        "                  q_values_SMDP2[state, optact] += alpha*(reward + gamma*np.max([q_values_SMDP2[next_state, action] for action in actions]) - q_values_SMDP2[state, optact])\n",
        "                  ufd2[state,optact] += 1\n",
        "                  \n",
        "                  if not optdone:\n",
        "                    q_values_SMDP2[state, action] += alpha*(reward + gamma*q_values_SMDP2[next_state, action] - q_values_SMDP2[state, action])\n",
        "                    ufd2[state,action] += 1\n",
        "                  else:\n",
        "                    q_values_SMDP2[state, action] += alpha*(reward + gamma*np.max([q_values_SMDP2[next_state, action] for action in actions]) - q_values_SMDP2[state, action])\n",
        "                    ufd2[state,action] += 1\n",
        "\n",
        "                  state = next_state\n",
        "\n",
        "\n",
        "          if action == 8: # action => move to G option\n",
        "\n",
        "              optdone = False\n",
        "              while (optdone == False) :\n",
        "                  \n",
        "                  optact,_ = move_to_Y(env,state) \n",
        "                  next_state, reward, done,_ = env.step(optact)\n",
        "                  _,optdone = move_to_Y(env,next_state) \n",
        "                  episode_reward+=reward\n",
        "\n",
        "                  q_values_SMDP2[state, optact] += alpha*(reward + gamma*np.max([q_values_SMDP2[next_state, action] for action in actions]) - q_values_SMDP2[state, optact])\n",
        "                  ufd2[state,optact] += 1\n",
        "                  \n",
        "                  if not optdone:\n",
        "                    q_values_SMDP2[state, action] += alpha*(reward + gamma*q_values_SMDP2[next_state, action] - q_values_SMDP2[state, action])\n",
        "                    ufd2[state,action] += 1\n",
        "                  else:\n",
        "                    q_values_SMDP2[state, action] += alpha*(reward + gamma*np.max([q_values_SMDP2[next_state, action] for action in actions]) - q_values_SMDP2[state, action])\n",
        "                    ufd2[state,action] += 1\n",
        "\n",
        "                  state = next_state\n",
        "\n",
        "\n",
        "          if action == 9: # action => move to B option\n",
        "\n",
        "              optdone = False\n",
        "              while (optdone == False) :\n",
        "                  \n",
        "                  optact,_ = move_to_B(env,state) \n",
        "                  next_state, reward, done,_ = env.step(optact)\n",
        "                  _,optdone = move_to_B(env,next_state)\n",
        "                  episode_reward+=reward \n",
        "\n",
        "                  q_values_SMDP2[state, optact] += alpha*(reward + gamma*np.max([q_values_SMDP2[next_state, action] for action in actions]) - q_values_SMDP2[state, optact])\n",
        "                  ufd2[state,optact] += 1\n",
        "                  \n",
        "                  if not optdone:\n",
        "                    q_values_SMDP2[state, action] += alpha*(reward + gamma*q_values_SMDP2[next_state, action] - q_values_SMDP2[state, action])\n",
        "                    ufd2[state,action] += 1\n",
        "                  else:\n",
        "                    q_values_SMDP2[state, action] += alpha*(reward + gamma*np.max([q_values_SMDP2[next_state, action] for action in actions]) - q_values_SMDP2[state, action])\n",
        "                    ufd2[state,action] += 1\n",
        "\n",
        "                  state = next_state\n",
        "      #wandb.log({'episode_reward': episode_reward})\n",
        "      Rewards.append(episode_reward)\n",
        "  return q_values_SMDP2, Rewards, ufd2\n",
        "\n",
        "\n",
        "# plt.plot(Rewards)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YEes43WgwUmo"
      },
      "outputs": [],
      "source": [
        "q_values_SMDP2, Rewards2, ufd2 = Intra(gamma, alpha, epsilon)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "np.mean(Rewards2[8000:])"
      ],
      "metadata": {
        "id": "ytulV1klDO9o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FhWtTBrMwUmo",
        "outputId": "0df2caba-01b4-4458-cc55-51a8c9ec9b2d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+\n",
            "|\u001b[35m\u001b[34;1m\u001b[43mR\u001b[0m\u001b[0m\u001b[0m: | : :G|\n",
            "| : | : : |\n",
            "| : : : : |\n",
            "| | : | : |\n",
            "|Y| : |B: |\n",
            "+---------+\n",
            "  (Dropoff)\n"
          ]
        }
      ],
      "source": [
        "drive_taxi(q_values_SMDP2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ihmxXXUWwUmo"
      },
      "outputs": [],
      "source": [
        "#case when passenger is at either R,G,Y,B\n",
        "#To visualise lets break the task into 2 parts: pick-up and drop\n",
        "op_SMDP_1 = [np.zeros((5,5)) for i in range(4)]  #Matrix to hold the optimal actions/options in each state\n",
        "op_SMDP_q1 = [np.zeros((5,5)) for i in range(4)]\n",
        "op_intra_1 = [np.zeros((5,5)) for i in range(4)] #cases where passengers are not in the taxi \n",
        "op_intra_q1 = [np.zeros((5,5)) for i in range(4)]\n",
        "\n",
        "op_SMDP_2 = [np.zeros((5,5)) for i in range(4)]  #Matrix to hold the optimal actions/options in each state\n",
        "op_SMDP_q2 = [np.zeros((5,5)) for i in range(4)]\n",
        "op_intra_2 = [np.zeros((5,5)) for i in range(4)] #cases where passenger is in the taxi\n",
        "op_intra_q2 = [np.zeros((5,5)) for i in range(4)]\n",
        "\n",
        "for i in range(500): #iterating over all states\n",
        "  state = list(env.decode(i))\n",
        "  if state[2] != 4:\n",
        "    op_SMDP_1[state[2]][state[0],state[1]] = np.argmax(q_values_SMDP[i])\n",
        "    op_SMDP_q1[state[2]][state[0],state[1]] = np.amax(q_values_SMDP[i])\n",
        "    op_intra_1[state[2]][state[0],state[1]] = np.argmax(q_values_SMDP2[i])\n",
        "    op_intra_q1[state[2]][state[0],state[1]] = np.amax(q_values_SMDP2[i])    \n",
        "  else:\n",
        "    op_SMDP_2[state[3]][state[0],state[1]] = np.argmax(q_values_SMDP[i])\n",
        "    op_SMDP_q2[state[3]][state[0],state[1]] = np.amax(q_values_SMDP[i])\n",
        "    op_intra_2[state[3]][state[0],state[1]] = np.argmax(q_values_SMDP2[i])\n",
        "    op_intra_q2[state[3]][state[0],state[1]] = np.amax(q_values_SMDP2[i]) \n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def visualise_q(m1,m2):\n",
        "  fig, ax = plt.subplots(figsize = (10,10))\n",
        "  im = ax.imshow(m1, extent=[0, 10, 0, 10])\n",
        "  ax.grid(which='major', axis='both', linestyle='-', color='k', linewidth=2)\n",
        "  divider = make_axes_locatable(ax)\n",
        "  cax = divider.append_axes('right', size='5%', pad=0.05)\n",
        "  fig.colorbar(im, cax=cax, orientation='vertical')\n",
        "  def x_direct(a):\n",
        "    if a in [0,1,4,5,6,7,8,9]:\n",
        "        return 0\n",
        "    elif a in [2]:\n",
        "        return 1\n",
        "    else:\n",
        "        return -1\n",
        "  def y_direct(a):\n",
        "    if a in [2,3,4,5,6,7,8,9]:\n",
        "      return 0\n",
        "    elif a in [1]:\n",
        "        return 1\n",
        "    else:\n",
        "        return -1\n",
        "  policyx = np.vectorize(x_direct)(m2)\n",
        "  policyy = np.vectorize(y_direct)(m2)\n",
        "  idx = 2*np.indices((5,5))\n",
        "  ax.quiver(idx[1][::-1].ravel()+ 1, idx[0][::-1].ravel()+1, policyx.ravel(), policyy.ravel(), pivot=\"middle\", color='red')\n",
        "\n",
        "  for i in range(5):\n",
        "    for j in range(5):\n",
        "      if m2[i][j] == 4:  #pick-rectangle\n",
        "        #rect = patches.Rectangle((idx[1][::-1][i][j]+0.5, idx[0][::-1][i][j]+0.5), 1, 1, linewidth=0.5, edgecolor='r', facecolor='r')\n",
        "        #ax.add_patch(rect)\n",
        "        ax.text(idx[1][::-1][i][j]+0.6, idx[0][::-1][i][j]+0.6 ,'Pick',color = 'r', fontsize= 20)\n",
        "      if m2[i][j] == 5:  #drop-Circle\n",
        "        #circ = patches.Circle((idx[1][::-1][i][j]+1, idx[0][::-1][i][j]+1), 0.5, linewidth=0.5, edgecolor='r', facecolor='r')\n",
        "        #ax.add_patch(circ)\n",
        "        ax.text(idx[1][::-1][i][j]+0.6, idx[0][::-1][i][j]+0.6 ,'Drop',color = 'r', fontsize= 20)\n",
        "      if m2[i][j] == 6:\n",
        "        ax.text(idx[1][::-1][i][j]+0.9, idx[0][::-1][i][j]+0.9 ,'R',color = 'r', fontsize= 35)\n",
        "\n",
        "      if m2[i][j] == 7:\n",
        "        ax.text(idx[1][::-1][i][j]+0.9, idx[0][::-1][i][j]+0.9 ,'Y',color = 'r', fontsize= 35)\n",
        "\n",
        "      if m2[i][j] == 8:\n",
        "        ax.text(idx[1][::-1][i][j]+0.9, idx[0][::-1][i][j]+0.9 ,'G',color = 'r', fontsize= 35)\n",
        "\n",
        "      if m2[i][j] == 9:\n",
        "        ax.text(idx[1][::-1][i][j]+0.9, idx[0][::-1][i][j]+0.9 ,'B',color = 'r', fontsize= 35)        \n",
        "\n",
        "  fig.tight_layout()\n"
      ],
      "metadata": {
        "id": "e8C--n1FRU1d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "HIozuLjFPfP0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h3> Now we experiment with another set of options </h3>"
      ],
      "metadata": {
        "id": "Zc6wW04pay8F"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0SfcOvScbskn"
      },
      "outputs": [],
      "source": [
        "# We are defining four more options here\n",
        "# Option 1 go to highway \n",
        "# Option 2 move left on highway\n",
        "# Option 3 move right on highway\n",
        "\n",
        "def goto_hw(env,state):\n",
        "    \n",
        "    ds = list(env.decode(state))[:2]\n",
        "    optdone = False\n",
        "    optact = h_mat[ds[0],ds[1]]\n",
        "    \n",
        "    if (ds[0] == 2):    #termination condition(if row==2)\n",
        "        optdone = True\n",
        "    \n",
        "    return [optact,optdone]\n",
        "\n",
        "def left_hw(env,state):\n",
        "\n",
        "    ds = list(env.decode(state))[:2]\n",
        "    optdone = False\n",
        "    optact = 3\n",
        "    \n",
        "    if (ds == [2,0]):     #termination condition\n",
        "        optdone = True\n",
        "    \n",
        "    return [optact,optdone]\n",
        "    \n",
        "def right_hw(env,state):\n",
        "    \n",
        "    ds = list(env.decode(state))[:2]\n",
        "    optdone = False\n",
        "    optact = 2\n",
        "    \n",
        "    if (ds == [2,3]):     #termination condition\n",
        "        optdone = True\n",
        "    \n",
        "    return [optact,optdone]\n",
        "    \n",
        "\n",
        "\n",
        "#Now the new action space will contain\n",
        "#Primitive Actions: [\"south\", \"north\", \"east\", \"west\",\"pick\",\"drop\"]\n",
        "#Options: [\"move_to_R\",\"move_to_Y\",\"move_to_G\",\"move_to_B\"]\n",
        "#Total Actions :[\"south\", \"north\", \"east\", \"west\",\"pick\",\"drop\",\"goto_hw\",\"left_hw\",\"right_hw\"]\n",
        "#Corresponding to [0,1,2,3,4,5,6,7,8]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AaOISyeebskt"
      },
      "outputs": [],
      "source": [
        "  \n",
        "#epsilon-greedy action selection function\n",
        "seed = 36\n",
        "rg = np.random.RandomState(seed)\n",
        "\n",
        "def egreedy_policy(q_values,state,epsilon):\n",
        "    nstate = list(env.decode(state))[:2]\n",
        "    if nstate[0]!=2:\n",
        "      actions = [0,1,2,3,4,5,6]\n",
        "    if nstate == [2,0]:\n",
        "      actions = [0,1,2,3,4,5,8]\n",
        "    if nstate == [2,1] or nstate ==[2,2] :\n",
        "      actions = [0,1,2,3,4,5,7,8]\n",
        "    if nstate == [2,3] or nstate ==[2,4] :\n",
        "      actions = [0,1,2,3,4,5,7]\n",
        "\n",
        "    if rg.rand() < epsilon:\n",
        "        return rg.choice(actions)\n",
        "    else:\n",
        "        #max = np.max(q_values[state]) \n",
        "        #return rg.choice(np.where(q_values[state] == max)[0])\n",
        "        return np.argmax(q_values[state])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_q():\n",
        "  q_values_SMDP = np.zeros((500,9))\n",
        "  for i in range(500):\n",
        "    pos = list(env.decode(i))[:2]\n",
        "    if pos[0] != 2:\n",
        "      q_values_SMDP[i,7] = -100\n",
        "      q_values_SMDP[i,8] = -100\n",
        "    \n",
        "    if pos == [2,0]:\n",
        "      q_values_SMDP[i,7] = -100\n",
        "      q_values_SMDP[i,6] = -100  \n",
        "\n",
        "    if pos == [2,1] or pos ==[2,2] :\n",
        "      q_values_SMDP[i,6] = -100 \n",
        "\n",
        "    if pos == [2,3] or pos ==[2,4] : \n",
        "      q_values_SMDP[i,6] = -100 \n",
        "      q_values_SMDP[i,8] = -100 \n",
        "\n",
        "  return q_values_SMDP"
      ],
      "metadata": {
        "id": "a7hhHh_YGhec"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5pcK6rP9bsk0"
      },
      "outputs": [],
      "source": [
        "\n",
        "#### SMDP Q-Learning \n",
        "actions = [0,1,2,3,4,5,6,7,8]\n",
        "# Add parameters you might need here\n",
        "gamma = 0.9\n",
        "alpha = 0.2\n",
        "epsilon=0.01\n",
        "# Iterate over 1000 episodes\n",
        "def SMDP(gamma,alpha,epsilon):\n",
        "  q_values_SMDP =get_q()\n",
        "  ufd1 = np.zeros((500,9))#Update_Frequency Data structure\n",
        "  Rewards = []\n",
        "  for _ in range(10000):\n",
        "      state = env.reset()    \n",
        "      done = False\n",
        "      \n",
        "      # While episode is not over\n",
        "      episode_reward = 0\n",
        "      while not done:\n",
        "          \n",
        "          # Choose action        \n",
        "          action = egreedy_policy(q_values_SMDP, state, epsilon)\n",
        "          \n",
        "          # Checking if primitive action\n",
        "          if action < 6:\n",
        "              # Perform regular Q-Learning update for state-action pair\n",
        "\n",
        "              next_state, reward, done,_ = env.step(action)\n",
        "              q_values_SMDP[state, action] += alpha*(reward + gamma*np.max([q_values_SMDP[next_state, action] for action in actions]) - q_values_SMDP[state, action])\n",
        "              ufd1[state,action] += 1\n",
        "              state = next_state\n",
        "              episode_reward += reward\n",
        "\n",
        "          # Checking if action chosen is an option\n",
        "          reward_bar = 0\n",
        "          if action == 6: # action => goto_hw\n",
        "              \n",
        "              initial_state = np.copy(state)\n",
        "              optdone = False\n",
        "              count=0\n",
        "              while (optdone == False):\n",
        "                  \n",
        "                  optact,_ = goto_hw(env,state) \n",
        "                  #\n",
        "                  next_state, reward, done,_ = env.step(optact)\n",
        "\n",
        "                  _,optdone = goto_hw(env,next_state) \n",
        "                  \n",
        "                  reward_bar = reward_bar +  (gamma**count)*reward\n",
        "                  count+=1\n",
        "                  state = next_state\n",
        "                  episode_reward += reward\n",
        "\n",
        "              q_values_SMDP[initial_state, action] += alpha*(reward_bar + (gamma**count)*np.max([q_values_SMDP[state, action] for action in actions]) - q_values_SMDP[initial_state, action])\n",
        "              ufd1[initial_state,action] += 1\n",
        "                \n",
        "            \n",
        "          if action == 7: # action => left_hw option\n",
        "\n",
        "              initial_state = np.copy(state)\n",
        "              optdone = False\n",
        "              count=0\n",
        "              while (optdone == False):\n",
        "                  \n",
        "                  optact,_ = left_hw(env,state) \n",
        "\n",
        "                  next_state, reward, done,_ = env.step(optact)\n",
        "\n",
        "                  _,optdone = left_hw(env,next_state) \n",
        "\n",
        "                  reward_bar = reward_bar +  (gamma**count)*reward\n",
        "                  count+=1\n",
        "    \n",
        "                  state = next_state\n",
        "                  episode_reward += reward\n",
        "\n",
        "              q_values_SMDP[initial_state, action] += alpha*(reward_bar + (gamma**count)*np.max([q_values_SMDP[state, action] for action in actions]) - q_values_SMDP[initial_state, action])\n",
        "              ufd1[initial_state,action] += 1\n",
        "\n",
        "\n",
        "          if action == 8: # action => right_hw option\n",
        "\n",
        "              initial_state = np.copy(state)\n",
        "              optdone = False\n",
        "              count=0\n",
        "              while (optdone == False):\n",
        "                  \n",
        "                  optact,_ = right_hw(env,state) \n",
        "                  #\n",
        "                  next_state, reward, done,_ = env.step(optact)\n",
        "\n",
        "                  \n",
        "                  _,optdone = right_hw(env,next_state) \n",
        "                  \n",
        "                  reward_bar = reward_bar +  (gamma**count)*reward\n",
        "                  count+=1\n",
        "\n",
        "                  state = next_state\n",
        "                  episode_reward += reward\n",
        "\n",
        "              q_values_SMDP[initial_state, action] += alpha*(reward_bar + (gamma**count)*np.max([q_values_SMDP[state, action] for action in actions]) - q_values_SMDP[initial_state, action])\n",
        "              ufd1[initial_state,action] += 1\n",
        "\n",
        "\n",
        "      Rewards.append(episode_reward)\n",
        "    \n",
        "  return q_values_SMDP,Rewards,ufd1\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "q_values1,rewards1,ufd1 = SMDP(gamma,alpha,epsilon)\n",
        "plt.figure(figsize=(9,6))\n",
        "plt.plot(rewards1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "id": "EUFzDSJrRxwz",
        "outputId": "9f7a0d18-2a71-4ce5-f287-bd0fffe1a189"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7fafee82e450>]"
            ]
          },
          "metadata": {},
          "execution_count": 389
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 648x432 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "np.mean(rewards1[8000:])"
      ],
      "metadata": {
        "id": "6duXbfEcSbPk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "47e404ac-6bb5-419c-b78b-bccfd28b6072"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "7.4725"
            ]
          },
          "metadata": {},
          "execution_count": 390
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vvgF92pBbsk8"
      },
      "outputs": [],
      "source": [
        "#### Intra-Option Q-Learning \n",
        "\n",
        "# Add parameters you might need here\n",
        "gamma = 0.93\n",
        "alpha = 0.2\n",
        "epsilon=0.001\n",
        "# Iterate over 1000 episodes\n",
        "def intra(gamma,alpha,epsilon):\n",
        "  Rewards = []\n",
        "  q_values_SMDP2 = get_q()\n",
        "  ufd2 = np.zeros((500,9))#Update_Frequency Data structure\n",
        "  for _ in range(10000):\n",
        "      state = env.reset()    \n",
        "      done = False\n",
        "\n",
        "      # While episode is not over\n",
        "      episode_reward = 0\n",
        "      while not done:\n",
        "\n",
        "          # Choose action        \n",
        "          action = egreedy_policy(q_values_SMDP2, state, epsilon)\n",
        "          \n",
        "          # Checking if primitive action\n",
        "          if action < 6:\n",
        "              # Perform regular Q-Learning update for state-action pair\n",
        "\n",
        "              next_state, reward, done,_ = env.step(action)\n",
        "              q_values_SMDP2[state, action] += alpha*(reward + gamma*np.max([q_values_SMDP2[next_state, action] for action in actions]) - q_values_SMDP2[state, action])\n",
        "              ufd2[state,action] += 1\n",
        "              episode_reward+=reward\n",
        "\n",
        "              state = next_state\n",
        "          \n",
        "          # Checking if action chosen is an option\n",
        "\n",
        "          if action == 6: # action => goto highway option\n",
        "\n",
        "              optdone = False\n",
        "              while (optdone == False) :\n",
        "                  \n",
        "                  optact,_ = goto_hw(env,state) \n",
        "                  next_state, reward, done,_ = env.step(optact)\n",
        "                  _,optdone = goto_hw(env,next_state) \n",
        "                  episode_reward+=reward\n",
        "\n",
        "                  q_values_SMDP2[state, optact] += alpha*(reward + gamma*np.max([q_values_SMDP2[next_state, action] for action in actions]) - q_values_SMDP2[state, optact])\n",
        "                  ufd2[state,optact] += 1\n",
        "                  \n",
        "                  if not optdone:\n",
        "                    q_values_SMDP2[state, action] += alpha*(reward + gamma*q_values_SMDP2[next_state, action] - q_values_SMDP2[state, action])\n",
        "                    ufd2[state,action] += 1\n",
        "                  else:\n",
        "                    q_values_SMDP2[state, action] += alpha*(reward + gamma*np.max([q_values_SMDP2[next_state, action] for action in actions]) - q_values_SMDP2[state, action])\n",
        "                    ufd2[state,action] += 1\n",
        "\n",
        "                  state = next_state\n",
        "\n",
        "              \n",
        "\n",
        "                \n",
        "            \n",
        "          if action == 7: # action => move left of highway option\n",
        "\n",
        "              optdone = False\n",
        "              while (optdone == False) :\n",
        "                  \n",
        "                  optact,_ = left_hw(env,state) \n",
        "                  next_state, reward, done,_ = env.step(optact)\n",
        "                  _,optdone = left_hw(env,next_state) \n",
        "                  episode_reward+=reward\n",
        "\n",
        "                  q_values_SMDP2[state, optact] += alpha*(reward + gamma*np.max([q_values_SMDP2[next_state, action] for action in actions]) - q_values_SMDP2[state, optact])\n",
        "                  ufd2[state,optact] += 1\n",
        "                  \n",
        "                  if not optdone:\n",
        "                    q_values_SMDP2[state, action] += alpha*(reward + gamma*q_values_SMDP2[next_state, action] - q_values_SMDP2[state, action])\n",
        "                    ufd2[state,action] += 1\n",
        "                  else:\n",
        "                    q_values_SMDP2[state, action] += alpha*(reward + gamma*np.max([q_values_SMDP2[next_state, action] for action in actions]) - q_values_SMDP2[state, action])\n",
        "                    ufd2[state,action] += 1\n",
        "\n",
        "                  state = next_state\n",
        "\n",
        "\n",
        "          if action == 8: # action => move right of highway option\n",
        "\n",
        "              optdone = False\n",
        "              while (optdone == False) :\n",
        "                  \n",
        "                  optact,_ = right_hw(env,state) \n",
        "                  next_state, reward, done,_ = env.step(optact)\n",
        "                  _,optdone = right_hw(env,next_state) \n",
        "                  episode_reward+=reward\n",
        "\n",
        "                  q_values_SMDP2[state, optact] += alpha*(reward + gamma*np.max([q_values_SMDP2[next_state, action] for action in actions]) - q_values_SMDP2[state, optact])\n",
        "                  ufd2[state,optact] += 1\n",
        "                  \n",
        "                  if not optdone:\n",
        "                    q_values_SMDP2[state, action] += alpha*(reward + gamma*q_values_SMDP2[next_state, action] - q_values_SMDP2[state, action])\n",
        "                    ufd2[state,action] += 1\n",
        "                  else:\n",
        "                    q_values_SMDP2[state, action] += alpha*(reward + gamma*np.max([q_values_SMDP2[next_state, action] for action in actions]) - q_values_SMDP2[state, action])\n",
        "                    ufd2[state,action] += 1\n",
        "\n",
        "                  state = next_state\n",
        "\n",
        "\n",
        "      Rewards.append(episode_reward)\n",
        "\n",
        "  return q_values_SMDP2,Rewards,ufd2\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "q_values2,rewards2,ufd2 = intra(gamma,alpha,epsilon)\n",
        "plt.figure(figsize=(9,6))\n",
        "plt.plot(rewards2)"
      ],
      "metadata": {
        "id": "4i0IkGKOwSUW",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "outputId": "23f12dc7-5d6f-42e0-cb27-201079a67919"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7fafee7b0890>]"
            ]
          },
          "metadata": {},
          "execution_count": 386
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 648x432 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "np.mean(rewards2[8000:])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fkRNW400TLdU",
        "outputId": "3361b2b2-d63f-4b78-aedd-77704595324a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "7.949"
            ]
          },
          "metadata": {},
          "execution_count": 387
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#case when passenger is at either R,G,Y,B\n",
        "#To visualise lets break the task into 2 parts: pick-up and drop\n",
        "op_SMDP_1 = [np.zeros((5,5)) for i in range(4)]  #Matrix to hold the optimal actions/options in each state\n",
        "op_SMDP_q1 = [np.zeros((5,5)) for i in range(4)]\n",
        "ufd_SMDP_1 = [np.zeros((5,5)) for i in range(4)]\n",
        "op_intra_1 = [np.zeros((5,5)) for i in range(4)] #cases where passengers are not in the taxi \n",
        "op_intra_q1 = [np.zeros((5,5)) for i in range(4)]\n",
        "ufd_intra_1 = [np.zeros((5,5)) for i in range(4)]\n",
        "\n",
        "op_SMDP_2 = [np.zeros((5,5)) for i in range(4)]  #Matrix to hold the optimal actions/options in each state\n",
        "op_SMDP_q2 = [np.zeros((5,5)) for i in range(4)]\n",
        "ufd_SMDP_2 = [np.zeros((5,5)) for i in range(4)]\n",
        "op_intra_2 = [np.zeros((5,5)) for i in range(4)] #cases where passenger is in the taxi\n",
        "op_intra_q2 = [np.zeros((5,5)) for i in range(4)]\n",
        "ufd_intra_2 = [np.zeros((5,5)) for i in range(4)]\n",
        "\n",
        "for i in range(500): #iterating over all states\n",
        "  state = list(env.decode(i))\n",
        "  if state[2] != 4:\n",
        "    op_SMDP_1[state[2]][state[0],state[1]] = np.argmax(q_values1[i])\n",
        "    op_SMDP_q1[state[2]][state[0],state[1]] = np.amax(q_values1[i])\n",
        "    ufd_SMDP_1[state[2]][state[0],state[1]] = np.sum(ufd1[i])\n",
        "    op_intra_1[state[2]][state[0],state[1]] = np.argmax(q_values2[i])\n",
        "    op_intra_q1[state[2]][state[0],state[1]] = np.amax(q_values2[i])\n",
        "    ufd_intra_1[state[2]][state[0],state[1]] = np.sum(ufd2[i])\n",
        "  else:\n",
        "    op_SMDP_2[state[3]][state[0],state[1]] = np.argmax(q_values1[i])\n",
        "    op_SMDP_q2[state[3]][state[0],state[1]] = np.amax(q_values1[i])\n",
        "    ufd_SMDP_2[state[3]][state[0],state[1]] = np.sum(ufd1[i])\n",
        "    op_intra_2[state[3]][state[0],state[1]] = np.argmax(q_values2[i])\n",
        "    op_intra_q2[state[3]][state[0],state[1]] = np.amax(q_values2[i])\n",
        "    ufd_intra_2[state[3]][state[0],state[1]] = np.sum(ufd2[i])\n"
      ],
      "metadata": {
        "id": "FuheQxrFhcIa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def visualise_q(m1,m2):\n",
        "  fig, ax = plt.subplots(figsize = (10,10))\n",
        "  im = ax.imshow(m1, extent=[0, 10, 0, 10])\n",
        "  ax.grid(which='major', axis='both', linestyle='-', color='k', linewidth=2)\n",
        "  divider = make_axes_locatable(ax)\n",
        "  cax = divider.append_axes('right', size='5%', pad=0.05)\n",
        "  fig.colorbar(im, cax=cax, orientation='vertical')\n",
        "  def x_direct(a):\n",
        "    if a in [0,1,4,5,6,7,8]:\n",
        "        return 0\n",
        "    elif a in [2]:\n",
        "        return 1\n",
        "    else:\n",
        "        return -1\n",
        "  def y_direct(a):\n",
        "    if a in [2,3,4,5,6,7,8]:\n",
        "      return 0\n",
        "    elif a in [1]:\n",
        "        return 1\n",
        "    else:\n",
        "        return -1\n",
        "  policyx = np.vectorize(x_direct)(m2)\n",
        "  policyy = np.vectorize(y_direct)(m2)\n",
        "  idx = 2*np.indices((5,5))\n",
        "  ax.quiver(idx[1][::-1].ravel()+ 1, idx[0][::-1].ravel()+1, policyx.ravel(), policyy.ravel(), pivot=\"middle\", color='red')\n",
        "\n",
        "  for i in range(5):\n",
        "    for j in range(5):\n",
        "      if m2[i][j] == 4:  #pick-rectangle\n",
        "        #rect = patches.Rectangle((idx[1][::-1][i][j]+0.5, idx[0][::-1][i][j]+0.5), 1, 1, linewidth=0.5, edgecolor='r', facecolor='r')\n",
        "        #ax.add_patch(rect)\n",
        "        ax.text(idx[1][::-1][i][j]+0.6, idx[0][::-1][i][j]+0.6 ,'Pick',color = 'r', fontsize= 20)\n",
        "      if m2[i][j] == 5:  #drop-Circle\n",
        "        #circ = patches.Circle((idx[1][::-1][i][j]+1, idx[0][::-1][i][j]+1), 0.5, linewidth=0.5, edgecolor='r', facecolor='r')\n",
        "        #ax.add_patch(circ)\n",
        "        ax.text(idx[1][::-1][i][j]+0.6, idx[0][::-1][i][j]+0.6 ,'Drop',color = 'r', fontsize= 20)\n",
        "      if m2[i][j] == 6:\n",
        "        ax.text(idx[1][::-1][i][j]+0.9, idx[0][::-1][i][j]+0.9 ,'H',color = 'r', fontsize= 35)\n",
        "\n",
        "      if m2[i][j] == 7:\n",
        "        ax.text(idx[1][::-1][i][j]+0.9, idx[0][::-1][i][j]+0.9 ,'L',color = 'r', fontsize= 35)\n",
        "\n",
        "      if m2[i][j] == 8:\n",
        "        ax.text(idx[1][::-1][i][j]+0.9, idx[0][::-1][i][j]+0.9 ,'R',color = 'r', fontsize= 35)\n",
        "  \n",
        "  fig.tight_layout()\n",
        "  "
      ],
      "metadata": {
        "id": "tne6o0fbvfVy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def freq_heat(m1):\n",
        "  fig, ax = plt.subplots(figsize = (10,10))\n",
        "  im = ax.imshow(m1, extent=[0, 10, 0, 10])\n",
        "  ax.grid(which='major', axis='both', linestyle='-', color='k', linewidth=2)\n",
        "  divider = make_axes_locatable(ax)\n",
        "  cax = divider.append_axes('right', size='5%', pad=0.05)\n",
        "  fig.colorbar(im, cax=cax, orientation='vertical')"
      ],
      "metadata": {
        "id": "B8Lb5m-xaTvi"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}